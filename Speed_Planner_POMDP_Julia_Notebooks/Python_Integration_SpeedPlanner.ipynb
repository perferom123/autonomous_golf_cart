{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "using POMDPs\n",
    "using Distributions: Normal\n",
    "using Random\n",
    "import POMDPs: initialstate_distribution, actions, gen, discount, isterminal\n",
    "Random.seed!(1);\n",
    "using POMDPs, POMDPModels, POMDPSimulators, ARDESPOT, POMDPModelTools,POMDPPolicies\n",
    "using ParticleFilters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct goal_location\n",
    "    x:: Int64\n",
    "    y:: Int64\n",
    "end\n",
    "\n",
    "struct pedestrian_state\n",
    "    x:: Int64\n",
    "    y:: Int64\n",
    "    goal:: goal_location\n",
    "end\n",
    "\n",
    "struct cart_state\n",
    "    x:: Int64\n",
    "    y:: Int64\n",
    "    theta:: Int64\n",
    "    v:: Int64\n",
    "end    \n",
    "\n",
    "struct golfcart_observations\n",
    "    observed_human_positions:: Array{goal_location}\n",
    "end\n",
    "\n",
    "struct SP_POMDP_state\n",
    "    cart:: cart_state\n",
    "    pedestrians:: Array{pedestrian_state}\n",
    "    pedestrian_goals:: Array{goal_location}\n",
    "    path_covered_index:: Int64\n",
    "end\n",
    "\n",
    "struct human_goal_probability\n",
    "    distribution::Array{Float64}\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutable struct Speed_Planner_POMDP <: POMDPs.POMDP{SP_POMDP_state,Int,golfcart_observations}\n",
    "    discount_factor::Float64\n",
    "    step_size::Int64\n",
    "    collision_threshold::Float64\n",
    "    collision_reward::Int64\n",
    "    goal_reward::Int64\n",
    "    max_cart_speed::Int64\n",
    "    cart_goal_position::goal_location\n",
    "    starting_cart_state::cart_state\n",
    "    starting_human_states::Array{pedestrian_state}\n",
    "    fixed_goal_locations::Array{goal_location}\n",
    "    human_goals_prob_distribution::Array{human_goal_probability}\n",
    "    astar_path::Array{Tuple{Int64,Int64}}\n",
    "    start_path_index::Int64\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "function POMDPs.gen(m::Speed_Planner_POMDP, s, a, rng)\n",
    "    \n",
    "    # transition model\n",
    "    \n",
    "    function calculate_theta(current_state, previous_state)\n",
    "        theta = 0\n",
    "        x_diff = current_state[1] - previous_state[1]\n",
    "        y_diff = current_state[2] - previous_state[2]\n",
    "        if x_diff != 0\n",
    "            if x_diff < 0\n",
    "                theta = 90\n",
    "            else\n",
    "                theta = 270\n",
    "            end\n",
    "        end\n",
    "        if y_diff != 0\n",
    "            if y_diff < 0\n",
    "                theta = 0\n",
    "            else\n",
    "                theta = 180\n",
    "            end\n",
    "        end\n",
    "        return theta\n",
    "    end\n",
    "    \n",
    "    function update_human_state(human, human_goals, rng)\n",
    "        goal = human.goal\n",
    "        human_fixed_goals = copy(human_goals)\n",
    "        deleteat!(human_fixed_goals, findall(x -> x==goal, human_fixed_goals)[1])\n",
    "        rand_num = rand(rng)\n",
    "        function move_human_towards_goal(human, goal)\n",
    "            temp_human_x = human.x\n",
    "            temp_human_y = human.y\n",
    "            if temp_human_x < goal.x\n",
    "                temp_human_x = temp_human_x + 1\n",
    "            elseif temp_human_x > goal.x\n",
    "                temp_human_x = temp_human_x - 1\n",
    "            end\n",
    "\n",
    "            if temp_human_y < goal.y\n",
    "                temp_human_y = temp_human_y + 1\n",
    "            elseif temp_human_y > goal.y\n",
    "                temp_human_y = temp_human_y - 1\n",
    "            end\n",
    "            temp_human_x = clamp(temp_human_x,1,14)\n",
    "            temp_human_y = clamp(temp_human_y,1,30)\n",
    "            return pedestrian_state(temp_human_x, temp_human_y, goal), goal_location(temp_human_x, temp_human_y)\n",
    "        end\n",
    "        if rand_num <= 0.7\n",
    "            # move human towards goal\n",
    "            new_human,observed_location = move_human_towards_goal(human, goal)\n",
    "        elseif rand_num > 0.7 && rand_num <= 0.8\n",
    "            new_human,observed_location = move_human_towards_goal(human, human_fixed_goals[1])\n",
    "        elseif rand_num > 0.8 && rand_num <= 0.9\n",
    "            new_human,observed_location = move_human_towards_goal(human, human_fixed_goals[2])\n",
    "        elseif rand_num > 0.9\n",
    "            new_human,observed_location = move_human_towards_goal(human, human_fixed_goals[3])\n",
    "        end\n",
    "        return new_human,observed_location\n",
    "    end\n",
    "\n",
    "    new_pedestrians = pedestrian_state[]\n",
    "    observed_positions = goal_location[]\n",
    "    \n",
    "    # action 0\n",
    "    if a == 0\n",
    "        # kart state +2 steps based on path\n",
    "        # x = new state in path's X\n",
    "        # y = new state in path's Y\n",
    "        # theta = new states - one previous state {if change in x or change in y}\n",
    "        # v = v\n",
    "        new_v = s.cart.v + a\n",
    "        new_position = m.astar_path[clamp(s.path_covered_index + new_v,1,length(m.astar_path))]\n",
    "        new_theta = calculate_theta(new_position, m.astar_path[clamp(s.path_covered_index + new_v - 1,1,length(m.astar_path))])\n",
    "        cart_new_state = cart_state(new_position[1], new_position[2], new_theta, new_v)\n",
    "        \n",
    "        # pedestrians state +1 step in their path for all pedestrians\n",
    "        # change x\n",
    "        # change y\n",
    "        for human in s.pedestrians\n",
    "            new_human,observed_location = update_human_state(human, s.pedestrian_goals, rng)\n",
    "            push!(new_pedestrians, new_human)\n",
    "            push!(observed_positions, observed_location)\n",
    "        end\n",
    "        # path {need to change now/later based on A* from kart's current position to goal}\n",
    "        new_path_index = s.path_covered_index + new_v\n",
    "    \n",
    "    # action 1\n",
    "    elseif a == 1\n",
    "        # kart state +3 steps based on path\n",
    "        # x = new state in path's X\n",
    "        # y = new state in path's Y\n",
    "        # theta = new states - one previous state {if change in x or change in y}\n",
    "        # v = v +1\n",
    "        new_v = (s.cart.v + a) % 5\n",
    "        new_position = m.astar_path[clamp(s.path_covered_index + new_v,1,length(m.astar_path))]\n",
    "        new_theta = calculate_theta(new_position, m.astar_path[clamp(s.path_covered_index + new_v - 1,1,length(m.astar_path))])\n",
    "        cart_new_state = cart_state(new_position[1], new_position[2], new_theta, new_v)\n",
    "            \n",
    "        # pedestrians state +1 step in their path for all pedestrians\n",
    "        # change x\n",
    "        # change y\n",
    "        for human in s.pedestrians\n",
    "            new_human,observed_location = update_human_state(human, s.pedestrian_goals, rng)\n",
    "            push!(new_pedestrians, new_human)\n",
    "            push!(observed_positions, observed_location)\n",
    "        end\n",
    "        \n",
    "        # path {need to change now/later based on A* from kart's current position to goal}\n",
    "        new_path_index = s.path_covered_index + new_v\n",
    "        \n",
    "    # action -1\n",
    "    elseif a == -1\n",
    "        # kart state +1 steps based on path\n",
    "        # x = new state in path's X\n",
    "        # y = new state in path's Y\n",
    "        # theta = new states - one previous state {if change in x or change in y}\n",
    "        # v = v -1\n",
    "        new_v = s.cart.v + a\n",
    "        if new_v < 0\n",
    "            new_v = 0\n",
    "        end\n",
    "        new_position = m.astar_path[clamp(s.path_covered_index + new_v,1,length(m.astar_path))]\n",
    "        new_theta = calculate_theta(new_position, m.astar_path[clamp(s.path_covered_index + new_v - 1,1,length(m.astar_path))])\n",
    "        cart_new_state = cart_state(new_position[1], new_position[2], new_theta, new_v)\n",
    "        \n",
    "        # pedestrians state +1 step in their path for all pedestrians\n",
    "        # change x\n",
    "        # change y\n",
    "        for human in s.pedestrians\n",
    "            new_human,observed_location = update_human_state(human, s.pedestrian_goals, rng)\n",
    "            push!(new_pedestrians, new_human)\n",
    "            push!(observed_positions, observed_location)\n",
    "        end\n",
    "        \n",
    "        # path {need to change now/later based on A* from kart's current position to goal}\n",
    "        new_path_index = s.path_covered_index + new_v\n",
    "        \n",
    "    end\n",
    "    \n",
    "    # update the state object\n",
    "    sp = SP_POMDP_state(cart_new_state, new_pedestrians, s.pedestrian_goals, new_path_index)\n",
    "\n",
    "    # observation model\n",
    "    o = golfcart_observations(observed_positions)\n",
    "    \n",
    "    # reward model \n",
    "    # collision reward\n",
    "    function collision_reward(sp, coll_threshold)\n",
    "        total_reward = 0\n",
    "        cart_pose_x = sp.cart.x\n",
    "        cart_pose_y = sp.cart.y\n",
    "        for human in sp.pedestrians\n",
    "            #dist = ((human.x - cart_pose_x)^2 + (human.y - cart_pose_y)^2)^0.5\n",
    "            dist = abs(human.x - cart_pose_x) + abs(human.y - cart_pose_y)\n",
    "            if dist < coll_threshold\n",
    "                total_reward = total_reward + m.collision_reward\n",
    "            end\n",
    "        end\n",
    "        return total_reward\n",
    "    end\n",
    "    \n",
    "    # goal reward\n",
    "    function goal_reward(sp, s, goal_state_reward)\n",
    "        total_reward = -10\n",
    "        cart_new_pose_x = sp.cart.x\n",
    "        cart_new_pose_y = sp.cart.y\n",
    "        \n",
    "        cart_goal = m.cart_goal_position\n",
    "        #new_dist = ((cart_goal.x - cart_new_pose_x)^2 + (cart_goal.y - cart_new_pose_y)^2)^0.5\n",
    "        new_dist = abs(cart_goal.x - cart_new_pose_x) + abs(cart_goal.y - cart_new_pose_y)\n",
    "        \n",
    "        cart_old_pose_x = s.cart.x\n",
    "        cart_old_pose_y = s.cart.y\n",
    "        #old_dist = ((cart_goal.x - cart_old_pose_x)^2 + (cart_goal.y - cart_old_pose_y)^2)^0.5\n",
    "        old_dist = abs(cart_goal.x - cart_old_pose_x) + abs(cart_goal.y - cart_old_pose_y)\n",
    "        \n",
    "        if new_dist < old_dist && new_dist != 0 && new_dist<4\n",
    "            total_reward = goal_state_reward/new_dist\n",
    "        elseif new_dist == 0\n",
    "            total_reward = goal_state_reward\n",
    "        end\n",
    "        return total_reward\n",
    "    end\n",
    "    \n",
    "    # speed reward\n",
    "    function speed_reward(sp, max_speed)\n",
    "        return (sp.cart.v - max_speed)/max_speed\n",
    "    end\n",
    "    \n",
    "    r = collision_reward(sp, m.collision_threshold) + goal_reward(sp, s, m.goal_reward) + speed_reward(sp, m.max_cart_speed)\n",
    "    #@show(\"Action is \", a)\n",
    "    #@show(s)\n",
    "    # create and return a NamedTuple\n",
    "    return (sp=sp, o=o, r=r)\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Discount and terminal state function\n",
    "\n",
    "function isgoalstate(s,cart_goal)\n",
    "    cart_x = s.cart.x\n",
    "    cart_y = s.cart.y\n",
    "    if(cart_goal.x == cart_x && cart_goal.y == cart_y)\n",
    "        return true\n",
    "    end\n",
    "    for human in s.pedestrians\n",
    "        if(cart_x == human.x && cart_y == human.y)\n",
    "            #display(\"Collision\")\n",
    "            return true\n",
    "        end\n",
    "    end\n",
    "    return false\n",
    "end\n",
    "\n",
    "discount(p::Speed_Planner_POMDP) = p.discount_factor\n",
    "isterminal(p::Speed_Planner_POMDP, s::SP_POMDP_state) = isgoalstate(s,p.cart_goal_position);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "actions (generic function with 17 methods)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Action Space for the POMDP\n",
    "actions(::Speed_Planner_POMDP) = [-1, 0, 1] # Decelerate Maintain Accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "initialstate_distribution (generic function with 9 methods)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Initial state distribution for the POMDP\n",
    "\n",
    "function initialstate_distribution(m::Speed_Planner_POMDP)\n",
    "    initial_cart_state = m.starting_cart_state\n",
    "    all_human_goal_locations = m.fixed_goal_locations\n",
    "    initial_human_states = m.starting_human_states\n",
    "    initial_path_start_index = m.start_path_index\n",
    "    initial_human_goal_probability = m.human_goals_prob_distribution\n",
    "    num_goals = length(all_human_goal_locations)\n",
    "    \n",
    "    all_256_possible_states = []\n",
    "    all_256_probability_values = Float64[]\n",
    "    \n",
    "    for goal_human1_index in (1:num_goals)\n",
    "        for goal_human2_index in (1:num_goals)\n",
    "            for goal_human3_index in (1:num_goals)\n",
    "                for goal_human4_index in (1:num_goals)\n",
    "                    sampled_human1_state = pedestrian_state(initial_human_states[1].x,initial_human_states[1].y,all_human_goal_locations[goal_human1_index])\n",
    "                    sampled_human2_state = pedestrian_state(initial_human_states[2].x,initial_human_states[2].y,all_human_goal_locations[goal_human2_index])\n",
    "                    sampled_human3_state = pedestrian_state(initial_human_states[3].x,initial_human_states[3].y,all_human_goal_locations[goal_human3_index])\n",
    "                    sampled_human4_state = pedestrian_state(initial_human_states[4].x,initial_human_states[4].y,all_human_goal_locations[goal_human4_index])\n",
    "                    sampled_humans = [sampled_human1_state, sampled_human2_state, sampled_human3_state, sampled_human4_state]                    \n",
    "                    generated_state = SP_POMDP_state(initial_cart_state,sampled_humans,all_human_goal_locations,initial_path_start_index)\n",
    "                    push!(all_256_possible_states,generated_state)\n",
    "                    \n",
    "                    human1_prob = initial_human_goal_probability[1].distribution[goal_human1_index]\n",
    "                    human2_prob = initial_human_goal_probability[2].distribution[goal_human2_index]\n",
    "                    human3_prob = initial_human_goal_probability[3].distribution[goal_human3_index]\n",
    "                    human4_prob = initial_human_goal_probability[4].distribution[goal_human4_index]\n",
    "                    probability_for_generated_state =  human1_prob*human2_prob*human3_prob*human4_prob\n",
    "                    push!(all_256_probability_values,probability_for_generated_state)\n",
    "                end\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "    d = SparseCat(all_256_possible_states, all_256_probability_values)\n",
    "    #@show(eltype(d.probs))               \n",
    "    return d\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "golf_cart_upper_bound (generic function with 1 method)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Upper bound for DESPOT\n",
    "\n",
    "function golf_cart_upper_bound(m, b)\n",
    "    value_sum = 0.0\n",
    "    function is_collision_state(s)\n",
    "        is_collision_flag = false\n",
    "        for human in s.pedestrians\n",
    "            #dist = ((human.x - s.cart.x)^2 + (human.y - s.cart.y)^2)^0.5\n",
    "            dist = abs(human.x - s.cart.x) + abs(human.y - s.cart.y)\n",
    "            if dist < m.collision_threshold\n",
    "                is_collision_flag = true\n",
    "            end\n",
    "        end\n",
    "        return is_collision_flag\n",
    "    end\n",
    "    function time_to_goal(s)\n",
    "        curr_vel = m.max_cart_speed\n",
    "        remaining_path_length = length(m.astar_path) - s.path_covered_index\n",
    "        time_needed_at_curr_vel = ceil(remaining_path_length/curr_vel)\n",
    "        return time_needed_at_curr_vel\n",
    "    end\n",
    "    for (s, w) in weighted_particles(b)\n",
    "        if(s.cart.x == 7 && s.cart.y==1)\n",
    "            value_sum += w*m.goal_reward\n",
    "        elseif (is_collision_state(s))\n",
    "            value_sum += w*m.collision_reward*(-1)\n",
    "        else\n",
    "            value_sum += w*((discount(m)^time_to_goal(s))*m.goal_reward)\n",
    "            #value_sum += w*m.goal_reward \n",
    "        end\n",
    "    end\n",
    "    #@show(value_sum)\n",
    "    return (value_sum)/weight_sum(b)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "get_best_possible_action (generic function with 1 method)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function get_best_possible_action(cart_start_state_list,cart_goal_position,pedestrians_list,possible_goal_positions,\n",
    "        initial_human_goal_distribution_list,given_astar_path)\n",
    "    \n",
    "    g1 =  goal_location(possible_goal_positions[1][1],possible_goal_positions[1][2])\n",
    "    g2 =  goal_location(possible_goal_positions[2][1],possible_goal_positions[2][2])\n",
    "    g3 =  goal_location(possible_goal_positions[3][1],possible_goal_positions[3][2])\n",
    "    g4 =  goal_location(possible_goal_positions[4][1],possible_goal_positions[4][2])\n",
    "    all_goals_list = [g1,g2,g3,g4]\n",
    "\n",
    "    cart_start_state = cart_state(cart_start_state_list[1],cart_start_state_list[2],\n",
    "        cart_start_state_list[3],cart_start_state_list[4])\n",
    "    cart_goal = goal_location(cart_goal_position[1],cart_goal_position[2])\n",
    "\n",
    "    ps1 = pedestrian_state(pedestrians_list[1][1],pedestrians_list[1][2], \n",
    "        goal_location(pedestrians_list[1][3][1],pedestrians_list[1][3][2]))\n",
    "    ps2 = pedestrian_state(pedestrians_list[2][1],pedestrians_list[2][2], \n",
    "        goal_location(pedestrians_list[2][3][1],pedestrians_list[2][3][2]))\n",
    "    ps3 = pedestrian_state(pedestrians_list[3][1],pedestrians_list[3][2], \n",
    "        goal_location(pedestrians_list[3][3][1],pedestrians_list[3][3][2]))\n",
    "    ps4 = pedestrian_state(pedestrians_list[4][1],pedestrians_list[4][2],\n",
    "        goal_location(pedestrians_list[4][3][1],pedestrians_list[4][3][2]))\n",
    "    human_state_start_list = [ps1,ps2,ps3,ps4]\n",
    "\n",
    "    h1_dis = human_goal_probability(initial_human_goal_distribution_list[1])\n",
    "    h2_dis = human_goal_probability(initial_human_goal_distribution_list[2])\n",
    "    h3_dis = human_goal_probability(initial_human_goal_distribution_list[3])\n",
    "    h4_dis = human_goal_probability(initial_human_goal_distribution_list[4])\n",
    "    human_dis_list = [h1_dis,h2_dis,h3_dis,h4_dis]\n",
    "\n",
    "    robot_path = Tuple{Int64,Int64}[]\n",
    "    \n",
    "    for position in given_astar_path\n",
    "        push!(robot_path,(position[1],position[2]))\n",
    "    end\n",
    "            \n",
    "    golfcart_pomdp() = Speed_Planner_POMDP(0.9,1,2,-10,100,5,cart_goal,cart_start_state,\n",
    "        human_state_start_list,all_goals_list,human_dis_list,robot_path,1)    \n",
    "    \n",
    "    m = golfcart_pomdp()\n",
    "    #solver = DESPOTSolver(bounds=(-20.0, 1000.0))\n",
    "    #solver = DESPOTSolver(bounds=(DefaultPolicyLB(RandomSolver()), golf_cart_upper_bound))\n",
    "    #solver = DESPOTSolver(bounds=IndependentBounds(DefaultPolicyLB(RandomSolver()), \n",
    "    #        golf_cart_upper_bound, check_terminal=true))\n",
    "    \n",
    "    solver = DESPOTSolver(bounds=IndependentBounds(DefaultPolicyLB(FunctionPolicy(b->-1)), golf_cart_upper_bound, check_terminal=true))\n",
    "\n",
    "    planner = solve(solver, m)\n",
    "    b = initialstate_distribution(m);\n",
    "    a = action(planner, b) \n",
    "    return a\n",
    "    \n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action_to_be_taken = 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cart_start_state_list = [8,30,0,2]\n",
    "cart_goal_position = [7,1]\n",
    "pedestrians_list = [[4,3,[14,30]], [3,22,[14,1]], [11,22,[1,30]], [12,4,[1,1]]]\n",
    "possible_goal_positions = [[1,1],[1,30],[14,30],[14,1]]\n",
    "initial_human_goal_distribution_list = [[0.15,0.3,0.5,0.05], [0.1,0.3,0.1,0.5], \n",
    "    [0.5,0.25,0.1,0.15], [0.05,0.5,0.35,0.1]]\n",
    "given_astar_path = [( 8, 30),( 7, 30),( 7, 29),( 7, 28),( 7, 27),( 7, 26),( 7, 25),\n",
    "    ( 7, 24),( 7, 23),( 7, 22),( 6, 22),( 6, 21),( 5, 21),( 5, 20),( 5, 19),\n",
    "             ( 5, 18),( 5, 17),( 5, 16),( 5, 15),( 5, 14),( 5, 13),( 5, 12),( 5, 11),\n",
    "             ( 5, 10),( 5,  9),( 5,  8),( 5,  7),( 5,  6),\n",
    "             ( 5,  5),( 5,  4),( 5,  3),( 5 , 2),( 5 , 1),( 6,  1),( 7,  1)];\n",
    "\n",
    "action_to_be_taken = get_best_possible_action(cart_start_state_list,cart_goal_position,pedestrians_list,possible_goal_positions,\n",
    "        initial_human_goal_distribution_list,given_astar_path);\n",
    "@show(action_to_be_taken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function get_best_possible_action(m::Speed_Planner_POMDP)\n",
    "   \n",
    "#     solver = DESPOTSolver(bounds=(-20.0, 1000.0))\n",
    "#     #solver = DESPOTSolver(bounds=(DefaultPolicyLB(RandomSolver()), golf_cart_upper_bound))\n",
    "#     planner = solve(solver, m);\n",
    "#     b = initialstate_distribution(m);\n",
    "#     a = action(planner, b) \n",
    "#     return a\n",
    "    \n",
    "# end\n",
    "#get_best_possible_action(golfcart_pomdp())"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.3.1",
   "language": "julia",
   "name": "julia-1.3"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
